{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2217184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "class AdaRoundFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha, gamma=0.01, zeta=1.1):\n",
    "        sigmoid = torch.sigmoid(alpha)\n",
    "        s = (sigmoid * (zeta - gamma)) + gamma\n",
    "        h_alpha = torch.clamp(s, 0, 1)\n",
    "        return torch.floor(x) + h_alpha\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None, None, None\n",
    "\n",
    "def adaround_weight(weight, alpha, gamma=0.01, zeta=1.1):\n",
    "    return AdaRoundFunction.apply(weight, alpha, gamma, zeta)\n",
    "\n",
    "def get_qparams(tensor, qmin, qmax, per_channel=False, channel_axis=0):\n",
    "    if per_channel:\n",
    "        dims = list(range(tensor.ndim))\n",
    "        dims.remove(channel_axis)\n",
    "        min_vals = tensor.amin(dim=dims, keepdim=True)\n",
    "        max_vals = tensor.amax(dim=dims, keepdim=True)\n",
    "    else:\n",
    "        min_vals = tensor.min()\n",
    "        max_vals = tensor.max()\n",
    "    scale = (max_vals - min_vals) / float(qmax - qmin)\n",
    "    scale = torch.clamp(scale, min=1e-8)\n",
    "    zero_point = torch.round(qmin - min_vals / scale)\n",
    "    return scale, zero_point\n",
    "\n",
    "def quantize_tensor(tensor, scale, zero_point, qmin, qmax):\n",
    "    q = torch.round(tensor / scale + zero_point)\n",
    "    q = torch.clamp(q, qmin, qmax)\n",
    "    return scale * (q - zero_point)\n",
    "\n",
    "def adaround_layer(layer, inputs, num_iterations=1000, beta_range=(20, 2), reg_param=0.01, per_channel=True):\n",
    "    assert isinstance(layer, nn.Conv2d), \"Only Conv2d supported\"\n",
    "\n",
    "    signed = True  # Można rozpoznać np. po typie aktywacji\n",
    "    qmin, qmax = (-128, 127) if signed else (0, 255)\n",
    "\n",
    "    weight = layer.weight.detach()\n",
    "    alpha = nn.Parameter(torch.zeros_like(weight))\n",
    "    scale_w, zp_w = get_qparams(weight, qmin, qmax, per_channel=True, channel_axis=0)\n",
    "\n",
    "    optimizer = torch.optim.Adam([alpha], lr=1e-2)\n",
    "    best_loss = float(\"inf\")\n",
    "    best_alpha = alpha.data.clone()\n",
    "\n",
    "    # Kwantyzacja wejścia (per-tensor)\n",
    "    scale_in, zp_in = get_qparams(inputs, qmin, qmax, per_channel=False)\n",
    "    inputs_q = quantize_tensor(inputs, scale_in, zp_in, qmin, qmax)\n",
    "\n",
    "    for step in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Kwantyzacja wag\n",
    "        weight_q = adaround_weight(weight / scale_w + zp_w, alpha)\n",
    "        weight_q = scale_w * (weight_q - zp_w)\n",
    "\n",
    "        # Forward oryginalny vs. kwantyzowany\n",
    "        out_fp = layer(inputs)\n",
    "        out_q = F.conv2d(inputs_q, weight_q, bias=layer.bias, stride=layer.stride,\n",
    "                         padding=layer.padding, dilation=layer.dilation, groups=layer.groups)\n",
    "\n",
    "        loss_data = F.mse_loss(out_q, out_fp)\n",
    "\n",
    "        beta = beta_range[0] * (1 - step / num_iterations) + beta_range[1] * (step / num_iterations)\n",
    "        h_alpha = torch.clamp(torch.sigmoid(alpha) * 1.2 - 0.1, 0, 1) #according to up and down\n",
    "        reg = torch.sum(1 - torch.abs(2 * h_alpha - 1) ** beta)\n",
    "\n",
    "        loss = loss_data + reg_param * reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_alpha = alpha.data.clone()\n",
    "\n",
    "    # Finalizacja\n",
    "    h_alpha = torch.clamp(torch.sigmoid(best_alpha) * 1.2 - 0.1, 0, 1)\n",
    "    final_w_q = scale_w * (torch.floor(weight / scale_w + zp_w) + h_alpha - zp_w)\n",
    "\n",
    "    return final_w_q\n",
    "\n",
    "# --- Wrapper ---\n",
    "\n",
    "class AdaRoundModelWrapper:\n",
    "    def __init__(self, model, sample_input):\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.sample_input = sample_input.to(next(self.model.parameters()).device)\n",
    "        self.device = self.sample_input.device\n",
    "        self.model.eval()\n",
    "\n",
    "    def apply_adaround_to_conv_layers(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                print(f\"[AdaRound] Processing layer: {name}\")\n",
    "\n",
    "                captured_input = None\n",
    "\n",
    "                def hook_fn(module, input, output):\n",
    "                    nonlocal captured_input\n",
    "                    captured_input = input[0].detach()\n",
    "\n",
    "                hook = module.register_forward_hook(hook_fn)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    _ = self.model(self.sample_input)\n",
    "\n",
    "                hook.remove()\n",
    "\n",
    "                if captured_input is None:\n",
    "                    raise RuntimeError(f\"Nie udało się przechwycić wejścia do warstwy {name}\")\n",
    "\n",
    "                # Kwantyzacja wag\n",
    "                quantized_weights = adaround_layer(module, captured_input)\n",
    "                module.weight.data.copy_(quantized_weights)\n",
    "                print(f\"[AdaRound] -> Done.\")\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        print(f\"[AdaRound] Quantized model saved to: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc3aa2",
   "metadata": {},
   "source": [
    "# Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23948cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada Round Accuracy\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model jak przy treningu\n",
    "model = resnet18(weights=None)\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.maxpool = nn.Identity()\n",
    "model.fc = nn.Linear(512, 10)\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/resnet18_cifar10_adaround.pth\"))\n",
    "model.eval().to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])  # Bez Normalize, bo nie było tego w treningu\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'✅ Accuracy of quantized (adaround): {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53019e",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b205e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OptimKernel",
   "language": "python",
   "name": "optimkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
