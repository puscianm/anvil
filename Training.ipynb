{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e26a33-dcb1-49fc-a879-b3ffdc82febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import os, pathlib\n",
    "\n",
    "base_path = pathlib.Path(\"/home/mpuscian/Desktop/repozytoria/MINI_projects/anvil/models/\")\n",
    "model_path = base_path.joinpath(\"cifar_model2.pth\")\n",
    "adaround_model_path = base_path.joinpath(\"adaround_model.pth\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = resnet18(weights=None)\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.maxpool = nn.Identity()\n",
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910aaca-8520-4361-bc61-2bd427641a7d",
   "metadata": {},
   "source": [
    "# Model without quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc601ff2-b919-4801-8c11-edfde6591f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[1/20] Loss: 1.3887 | Val Accuracy: 55.14%\n",
      "âœ… Nowy najlepszy model zapisany (55.14%)\n",
      "[2/20] Loss: 0.9243 | Val Accuracy: 64.82%\n",
      "âœ… Nowy najlepszy model zapisany (64.82%)\n",
      "[3/20] Loss: 0.6023 | Val Accuracy: 59.34%\n",
      "[4/20] Loss: 0.2969 | Val Accuracy: 66.20%\n",
      "âœ… Nowy najlepszy model zapisany (66.20%)\n",
      "[5/20] Loss: 0.1183 | Val Accuracy: 63.60%\n",
      "[6/20] Loss: 0.0643 | Val Accuracy: 64.16%\n",
      "[7/20] Loss: 0.0503 | Val Accuracy: 63.00%\n",
      "[8/20] Loss: 0.0639 | Val Accuracy: 62.18%\n",
      "[9/20] Loss: 0.0859 | Val Accuracy: 65.62%\n",
      "[10/20] Loss: 0.0624 | Val Accuracy: 65.60%\n",
      "[11/20] Loss: 0.0401 | Val Accuracy: 66.40%\n",
      "âœ… Nowy najlepszy model zapisany (66.40%)\n",
      "[12/20] Loss: 0.0393 | Val Accuracy: 65.02%\n",
      "[13/20] Loss: 0.0578 | Val Accuracy: 64.48%\n",
      "[14/20] Loss: 0.0470 | Val Accuracy: 64.72%\n",
      "[15/20] Loss: 0.0475 | Val Accuracy: 60.56%\n",
      "[16/20] Loss: 0.0433 | Val Accuracy: 63.92%\n",
      "[17/20] Loss: 0.0362 | Val Accuracy: 64.06%\n",
      "[18/20] Loss: 0.0483 | Val Accuracy: 66.40%\n",
      "[19/20] Loss: 0.0458 | Val Accuracy: 66.32%\n",
      "[20/20] Loss: 0.0335 | Val Accuracy: 65.84%\n",
      "ðŸŽ¯ DokÅ‚adnoÅ›Ä‡ na zbiorze testowym: 65.49%\n",
      "ðŸ“¦ Model zapisany do: /home/mpuscian/Desktop/repozytoria/MINI_projects/anvil/models/cifar_model2.pth\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 3. Datasets\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_size = int(0.9 * len(full_train_dataset))  # 45,000\n",
    "val_size = len(full_train_dataset) - train_size  # 5,000\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# 4. DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=6)\n",
    "val_dataset.dataset.transform = transform_test\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=6)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=6)\n",
    "\n",
    "# 5. Model\n",
    "cifar_model = copy.deepcopy(model)\n",
    "cifar_model = cifar_model.to(device)\n",
    "\n",
    "# 6. Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cifar_model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "\n",
    "# 7. Training with validation\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(20):\n",
    "    cifar_model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cifar_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    cifar_model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = cifar_model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "            val_total += targets.size(0)\n",
    "\n",
    "    val_acc = 100.0 * val_correct / val_total\n",
    "    print(f\"[{epoch+1}/20] Loss: {train_loss/len(train_loader):.4f} | Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(cifar_model.state_dict(), model_path)\n",
    "        print(f\"âœ… Nowy najlepszy model zapisany ({val_acc:.2f}%)\")\n",
    "\n",
    "# 8. Test best model\n",
    "cifar_model.load_state_dict(torch.load(model_path))\n",
    "cifar_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = cifar_model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"ðŸŽ¯ DokÅ‚adnoÅ›Ä‡ na zbiorze testowym: {accuracy:.2f}%\")\n",
    "print(f\"ðŸ“¦ Model zapisany do: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48495f54-4f99-40fa-a6b4-ef4e1cd885bc",
   "metadata": {},
   "source": [
    "# Adaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60f98166-7e09-4c23-9f86-2f9b77232178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AdaRound] Processing layer: conv1\n",
      "final_wq: tensor([[[[-0.1177, -0.0938, -0.1177],\n",
      "          [ 0.1894, -0.0598, -0.0787],\n",
      "          [-0.1001, -0.1315,  0.1869]],\n",
      "\n",
      "         [[-0.1089,  0.1542,  0.0636],\n",
      "          [ 0.1718,  0.0170,  0.1756],\n",
      "          [ 0.1277,  0.0120, -0.0233]],\n",
      "\n",
      "         [[ 0.1252, -0.0296,  0.1655],\n",
      "          [-0.1215,  0.1781,  0.1768],\n",
      "          [ 0.1202, -0.1277,  0.1026]]],\n",
      "\n",
      "\n",
      "        [[[-0.0574, -0.1573,  0.1916],\n",
      "          [ 0.0529, -0.0201, -0.1111],\n",
      "          [-0.1886,  0.1901, -0.0291]],\n",
      "\n",
      "         [[-0.0470,  0.0216,  0.0500],\n",
      "          [ 0.0410, -0.1036, -0.0753],\n",
      "          [ 0.0052, -0.1126,  0.1722]],\n",
      "\n",
      "         [[ 0.0216,  0.0544, -0.0112],\n",
      "          [ 0.0992,  0.0186,  0.0037],\n",
      "          [-0.0723,  0.1230, -0.0649]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0525, -0.0179,  0.0086],\n",
      "          [ 0.0897, -0.1017, -0.0937],\n",
      "          [-0.1642,  0.0392,  0.1163]],\n",
      "\n",
      "         [[ 0.0419, -0.0711, -0.1881],\n",
      "          [ 0.0525,  0.1296,  0.0126],\n",
      "          [ 0.1509,  0.1216, -0.0538]],\n",
      "\n",
      "         [[ 0.0831, -0.0778, -0.1416],\n",
      "          [-0.0166, -0.1376,  0.0525],\n",
      "          [ 0.0126,  0.1044, -0.0831]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.1858, -0.0110, -0.0771],\n",
      "          [-0.1755, -0.1550, -0.0272],\n",
      "          [ 0.1579, -0.0801, -0.1124]],\n",
      "\n",
      "         [[-0.1800,  0.1432, -0.1829],\n",
      "          [-0.1227,  0.1153,  0.0712],\n",
      "          [ 0.1917, -0.1491,  0.0977]],\n",
      "\n",
      "         [[ 0.0301, -0.0757, -0.0845],\n",
      "          [-0.0037,  0.0316, -0.0918],\n",
      "          [ 0.1447,  0.1682,  0.0948]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1775, -0.0438, -0.0755],\n",
      "          [-0.0007, -0.1790, -0.1416],\n",
      "          [-0.1890,  0.0942,  0.1718]],\n",
      "\n",
      "         [[ 0.0323, -0.0927, -0.1100],\n",
      "          [ 0.1574, -0.0855,  0.0568],\n",
      "          [-0.1416,  0.1445,  0.1373]],\n",
      "\n",
      "         [[ 0.1517, -0.1243, -0.0482],\n",
      "          [ 0.1732,  0.1287,  0.0568],\n",
      "          [-0.1200, -0.0956, -0.0726]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0811,  0.1496, -0.1332],\n",
      "          [-0.0409,  0.0439,  0.1213],\n",
      "          [-0.0915,  0.1421, -0.1823]],\n",
      "\n",
      "         [[ 0.0067,  0.0171,  0.0900],\n",
      "          [-0.0930, -0.1525,  0.0499],\n",
      "          [-0.0409, -0.1347,  0.0543]],\n",
      "\n",
      "         [[-0.0915,  0.0141, -0.0424],\n",
      "          [-0.1198,  0.0662,  0.1183],\n",
      "          [-0.1897,  0.1897,  0.1243]]]])\n",
      "[AdaRound] -> Done.\n"
     ]
    }
   ],
   "source": [
    "import anvil\n",
    "import anvil.adaround\n",
    "\n",
    "adaround_model = copy.deepcopy(model)\n",
    "adaround_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# StwÃ³rz wejÅ›cie testowe (np. batch 1 obrazka)\n",
    "sample_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Kwantyzuj\n",
    "adarounder = anvil.adaround.AdaRoundModelWrapper(adaround_model, sample_input)\n",
    "scale, zp = adarounder.apply_adaround_to_conv_layers()\n",
    "#adarounder.save_model(adaround_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46a4d271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 3, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaround_model.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "514277da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -70.,  -51.,  -70.],\n",
       "          [ 175.,  -24.,  -39.],\n",
       "          [ -56.,  -80.,  172.]],\n",
       "\n",
       "         [[ -62.,  147.,   74.],\n",
       "          [ 161.,   38.,  164.],\n",
       "          [ 126.,   34.,    5.]],\n",
       "\n",
       "         [[ 123.,    1.,  156.],\n",
       "          [ -72.,  166.,  165.],\n",
       "          [ 119.,  -78.,  105.]]],\n",
       "\n",
       "\n",
       "        [[[ -37., -104.,  129.],\n",
       "          [  36.,  -13.,  -74.],\n",
       "          [-126.,  128.,  -19.]],\n",
       "\n",
       "         [[ -31.,   15.,   34.],\n",
       "          [  28.,  -69.,  -49.],\n",
       "          [   5.,  -75.,  116.]],\n",
       "\n",
       "         [[  16.,   38.,   -7.],\n",
       "          [  68.,   13.,    3.],\n",
       "          [ -48.,   83.,  -43.]]],\n",
       "\n",
       "\n",
       "        [[[  27.,  -26.,   -7.],\n",
       "          [  55.,  -89.,  -84.],\n",
       "          [-136.,   16.,   75.]],\n",
       "\n",
       "         [[  19.,  -67., -154.],\n",
       "          [  26.,   84.,   -3.],\n",
       "          [ 101.,   78.,  -54.]],\n",
       "\n",
       "         [[  49.,  -71., -120.],\n",
       "          [ -25., -116.,   27.],\n",
       "          [  -4.,   66.,  -76.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 129.,   -5.,  -50.],\n",
       "          [-116., -103.,  -16.],\n",
       "          [ 111.,  -52.,  -73.]],\n",
       "\n",
       "         [[-120.,  101., -122.],\n",
       "          [ -81.,   82.,   52.],\n",
       "          [ 133.,  -98.,   70.]],\n",
       "\n",
       "         [[  23.,  -49.,  -54.],\n",
       "          [   0.,   25.,  -59.],\n",
       "          [ 102.,  118.,   67.]]],\n",
       "\n",
       "\n",
       "        [[[ 119.,  -35.,  -57.],\n",
       "          [  -5., -128., -102.],\n",
       "          [-136.,   62.,  115.]],\n",
       "\n",
       "         [[  19.,  -69.,  -81.],\n",
       "          [ 106.,  -63.,   36.],\n",
       "          [-102.,   97.,   91.]],\n",
       "\n",
       "         [[ 101.,  -90.,  -38.],\n",
       "          [ 117.,   86.,   35.],\n",
       "          [ -87.,  -71.,  -54.]]],\n",
       "\n",
       "\n",
       "        [[[  55.,  101.,  -89.],\n",
       "          [ -27.,   31.,   83.],\n",
       "          [ -60.,   96., -122.]],\n",
       "\n",
       "         [[   5.,   13.,   62.],\n",
       "          [ -61., -101.,   35.],\n",
       "          [ -26.,  -90.,   37.]],\n",
       "\n",
       "         [[ -61.,   11.,  -27.],\n",
       "          [ -79.,   46.,   80.],\n",
       "          [-126.,  129.,   85.]]]], grad_fn=<RoundBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round((adaround_model.conv1.weight/scale) - zp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78d4cfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(150.8979, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(\n",
    "    (adaround_model.conv1.weight)/scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ea1fb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.1177, -0.0938, -0.1179],\n",
       "          [ 0.1899, -0.0601, -0.0789],\n",
       "          [-0.1002, -0.1310,  0.1868]],\n",
       "\n",
       "         [[-0.1086,  0.1543,  0.0632],\n",
       "          [ 0.1722,  0.0176,  0.1762],\n",
       "          [ 0.1282,  0.0123, -0.0237]],\n",
       "\n",
       "         [[ 0.1251, -0.0293,  0.1657],\n",
       "          [-0.1209,  0.1783,  0.1771],\n",
       "          [ 0.1196, -0.1280,  0.1021]]],\n",
       "\n",
       "\n",
       "        [[[-0.0567, -0.1571,  0.1910],\n",
       "          [ 0.0523, -0.0208, -0.1115],\n",
       "          [-0.1893,  0.1900, -0.0296]],\n",
       "\n",
       "         [[-0.0477,  0.0215,  0.0499],\n",
       "          [ 0.0403, -0.1039, -0.0748],\n",
       "          [ 0.0059, -0.1131,  0.1720]],\n",
       "\n",
       "         [[ 0.0221,  0.0551, -0.0118],\n",
       "          [ 0.0998,  0.0179,  0.0032],\n",
       "          [-0.0728,  0.1226, -0.0652]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0528, -0.0178,  0.0081],\n",
       "          [ 0.0898, -0.1015, -0.0938],\n",
       "          [-0.1639,  0.0390,  0.1165]],\n",
       "\n",
       "         [[ 0.0420, -0.0713, -0.1879],\n",
       "          [ 0.0521,  0.1292,  0.0128],\n",
       "          [ 0.1511,  0.1210, -0.0540]],\n",
       "\n",
       "         [[ 0.0825, -0.0777, -0.1421],\n",
       "          [-0.0163, -0.1372,  0.0528],\n",
       "          [ 0.0124,  0.1046, -0.0837]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.1857, -0.0112, -0.0772],\n",
       "          [-0.1750, -0.1553, -0.0279],\n",
       "          [ 0.1586, -0.0806, -0.1122]],\n",
       "\n",
       "         [[-0.1806,  0.1439, -0.1834],\n",
       "          [-0.1230,  0.1155,  0.0714],\n",
       "          [ 0.1912, -0.1488,  0.0978]],\n",
       "\n",
       "         [[ 0.0296, -0.0763, -0.0844],\n",
       "          [-0.0041,  0.0318, -0.0912],\n",
       "          [ 0.1451,  0.1687,  0.0941]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1774, -0.0440, -0.0757],\n",
       "          [-0.0008, -0.1786, -0.1412],\n",
       "          [-0.1891,  0.0948,  0.1712]],\n",
       "\n",
       "         [[ 0.0326, -0.0927, -0.1103],\n",
       "          [ 0.1576, -0.0854,  0.0575],\n",
       "          [-0.1413,  0.1447,  0.1366]],\n",
       "\n",
       "         [[ 0.1510, -0.1238, -0.0485],\n",
       "          [ 0.1739,  0.1291,  0.0564],\n",
       "          [-0.1196, -0.0963, -0.0722]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0805,  0.1492, -0.1339],\n",
       "          [-0.0414,  0.0445,  0.1219],\n",
       "          [-0.0911,  0.1421, -0.1829]],\n",
       "\n",
       "         [[ 0.0064,  0.0176,  0.0904],\n",
       "          [-0.0930, -0.1521,  0.0505],\n",
       "          [-0.0404, -0.1353,  0.0540]],\n",
       "\n",
       "         [[-0.0921,  0.0148, -0.0420],\n",
       "          [-0.1196,  0.0664,  0.1179],\n",
       "          [-0.1890,  0.1905,  0.1248]]]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaround_model.conv1.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anvil",
   "language": "python",
   "name": "anvil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
